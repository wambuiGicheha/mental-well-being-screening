{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Social Media-Based Depression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mental health is an urgent issue globally, with depression affecting millions of individuals across all demographics. The internet, and particularly social media, has become a place where people often express their struggles, including depression. Early detection of depression symptons can be crucial in providing timely support or intervention. This projectâ€™s goal is to leverage data science to better understand and detect depressive expressions in online platforms, potentially paving the way for more proactive mental health support. Given the widespread use of platforms like Reddit, this research could benefit individuals by increasing awareness and intervention opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Understanding\n",
    "\n",
    "Datasource: https://www.kaggle.com/datasets/rishabhkausish/reddit-depression-dataset/data\n",
    "\n",
    "The dataset already includes several key features that can be used to analyze and predict depression indicators based on Reddit posts. Specifically, the data has 7 key columns:\n",
    "\n",
    "1. Subreddit : The subreddit where each post was made, with posts from \"Depression\" and \"SuicideWatch\" labeled as 1 for depression and posts from other subreddits labeled as 0 (non-depression).\n",
    "\n",
    "2. Title: The title of the Reddit post.\n",
    "\n",
    "3. Body: The full text of the Reddit post, which may contain valuable information for understanding the context, tone, and possible indicators of depression.\n",
    "\n",
    "4. Upvotes: Number of upvotes each post received, which may indicate the post's visibility or resonance with the community.\n",
    "\n",
    "5. Created_utc: The timestamp of when the post was created in UTC, which can help in analyzing temporal trends.\n",
    "\n",
    "6. Num_comments: The number of comments on each post, which could provide insights into community engagement.\n",
    "\n",
    "7. Label: The target variable indicating depression (1) or non-depression (0) based on subreddit. .... The raw data was collected from five Reddit subreddits (sub topics), categorized based on their content. These included: Teenagers, Depression, SuicideWatch, DeepThoughts, Happy\n",
    "\n",
    "Since the data is already collected from Reddit, with over 6 million rows, further data acquisition may not be necessary. Infact, the team proposes reducing the dataset to about 500,000 rows for the purpose of this project(and to save our laptops). However, if additional data is needed, we could plan to scrape Reddit for more recent posts using a tool that we could identify through further research, provided we comply with Reddit's data collection policies and privacy standards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GICHEHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GICHEHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GICHEHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from preprocessing import TextPreprocessor\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Day 18 of doing 50 push-ups</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.619357e+09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>There isnâ€™t a better feeling than finishing yo...</td>\n",
       "      <td>Then your teacher hits you with that â€œ Good jo...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.554103e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>LMAOOO I can only get this guy to talk to me i...</td>\n",
       "      <td>Yeahhh maybe not babe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.649342e+09</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>This isn't going to work out</td>\n",
       "      <td>NaN</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1.417630e+09</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Am I the only guy who found it hurtful as a ch...</td>\n",
       "      <td>\\n\\nLike... why? How is that funny? How does ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.569280e+09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Phew , close one</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.587389e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Listen to my favorite song part 2 ðŸ¤©ðŸ¤©ðŸ¤©</td>\n",
       "      <td>https://youtu.be/MtN1YnoL46Q</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.610744e+09</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Hard core cuddling</td>\n",
       "      <td>#moans</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.625997e+09</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Don't communicate with women.</td>\n",
       "      <td>I watched my friend go to high five a girl and...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.557873e+09</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Do you guys just ever want to be left alone?</td>\n",
       "      <td>Just, after a bunch of socialising I want to c...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.511478e+09</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              title  \\\n",
       "0  teenagers                        Day 18 of doing 50 push-ups   \n",
       "1  teenagers  There isnâ€™t a better feeling than finishing yo...   \n",
       "2  teenagers  LMAOOO I can only get this guy to talk to me i...   \n",
       "3  teenagers                       This isn't going to work out   \n",
       "4  teenagers  Am I the only guy who found it hurtful as a ch...   \n",
       "5  teenagers                                   Phew , close one   \n",
       "6  teenagers              Listen to my favorite song part 2 ðŸ¤©ðŸ¤©ðŸ¤©   \n",
       "7  teenagers                                 Hard core cuddling   \n",
       "8  teenagers                      Don't communicate with women.   \n",
       "9  teenagers       Do you guys just ever want to be left alone?   \n",
       "\n",
       "                                                body  upvotes   created_utc  \\\n",
       "0                                                NaN      4.0  1.619357e+09   \n",
       "1  Then your teacher hits you with that â€œ Good jo...      7.0  1.554103e+09   \n",
       "2                              Yeahhh maybe not babe      4.0  1.649342e+09   \n",
       "3                                                NaN    236.0  1.417630e+09   \n",
       "4   \\n\\nLike... why? How is that funny? How does ...      6.0  1.569280e+09   \n",
       "5                                                NaN     11.0  1.587389e+09   \n",
       "6                       https://youtu.be/MtN1YnoL46Q      9.0  1.610744e+09   \n",
       "7                                             #moans     41.0  1.625997e+09   \n",
       "8  I watched my friend go to high five a girl and...      7.0  1.557873e+09   \n",
       "9  Just, after a bunch of socialising I want to c...      5.0  1.511478e+09   \n",
       "\n",
       "   num_comments  label  \n",
       "0           4.0    0.0  \n",
       "1           NaN    0.0  \n",
       "2          12.0    0.0  \n",
       "3          33.0    0.0  \n",
       "4           4.0    0.0  \n",
       "5           NaN    0.0  \n",
       "6          11.0    0.0  \n",
       "7          19.0    0.0  \n",
       "8          11.0    0.0  \n",
       "9          10.0    0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset to use \n",
    "\n",
    "data = pd.read_csv('Data/reduced_reddit.csv', index_col=False)\n",
    "data.head(10) # Display the first few rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 98,826 posts from different subreddits: teenagers, depression, SuicideWatch, \n",
    "happy DeepThoughts, with details like title, body text, upvotes, comments, and a binary label (0 or 1).\n",
    "\n",
    "The upvotes range widely, indicating some posts are very popular, while others receive minimal attention.\n",
    "\n",
    "The label distribution is imbalanced, with approximately 19% of posts labeled 1, which could represent a classification problem, possibly predicting the likelihood of a post being categorized as \"important\" or \"engaging.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98826 entries, 0 to 98825\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   subreddit     98826 non-null  object \n",
      " 1   title         98826 non-null  object \n",
      " 2   body          80371 non-null  object \n",
      " 3   upvotes       98826 non-null  float64\n",
      " 4   created_utc   98826 non-null  float64\n",
      " 5   num_comments  94297 non-null  float64\n",
      " 6   label         98826 non-null  float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#check the dataset information \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 98,826 posts with 7 columns: subreddit, title, body, upvotes, creation time (UTC), number of comments, and a label.\n",
    "\n",
    "The body column has some missing values (~20% missing), and the num_comments column also has some missing data (about 5% missing). \n",
    "\n",
    "All other columns are complete with no missing data, and the dataset uses a mix of numeric and object data types.With this view we will have to handle the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>upvotes</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98826.000000</td>\n",
       "      <td>9.882600e+04</td>\n",
       "      <td>94297.000000</td>\n",
       "      <td>98826.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>62.315555</td>\n",
       "      <td>1.566569e+09</td>\n",
       "      <td>15.217069</td>\n",
       "      <td>0.194443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>937.347581</td>\n",
       "      <td>6.977745e+07</td>\n",
       "      <td>71.638197</td>\n",
       "      <td>0.395773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.202084e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.552254e+09</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.578218e+09</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.611584e+09</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>62899.000000</td>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>7880.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            upvotes   created_utc  num_comments         label\n",
       "count  98826.000000  9.882600e+04  94297.000000  98826.000000\n",
       "mean      62.315555  1.566569e+09     15.217069      0.194443\n",
       "std      937.347581  6.977745e+07     71.638197      0.395773\n",
       "min        4.000000  1.202084e+09      1.000000      0.000000\n",
       "25%        5.000000  1.552254e+09      4.000000      0.000000\n",
       "50%        7.000000  1.578218e+09      7.000000      0.000000\n",
       "75%       11.000000  1.611584e+09     14.000000      0.000000\n",
       "max    62899.000000  1.672531e+09   7880.000000      1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check on the description of numerical datatypes\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upvotes range widely from 4 to 62,899, with a mean of 62.32, while the number of comments also varies significantly from 1 to 7,880, with a mean of 15.22. \n",
    "\n",
    "The label column shows an imbalance, with only 19% of posts labeled as positive (1), indicating a predominance of neutral or negative posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = 'models/sentiments_pipeline.pkl'\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File exists\")\n",
    "else:\n",
    "    print(\"File does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit        0.000000\n",
       "title            0.000000\n",
       "body            18.674236\n",
       "upvotes          0.000000\n",
       "created_utc      0.000000\n",
       "num_comments     4.582802\n",
       "label            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if there are null values in percentage form\n",
    "data.isnull().sum() / len(data) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has missing values in the 'body' and 'num_comments' columns. \n",
    "\n",
    "Specifically, 18.67% of the 'body' entries and 4.58% of the 'num_comments' entries are missing, while the other columns ('subreddit', 'title', 'upvotes', 'created_utc', and 'label') have no missing values.\n",
    "\n",
    "We will need to handle the missing values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98826 entries, 0 to 98825\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   subreddit     98826 non-null  object \n",
      " 1   title         98826 non-null  object \n",
      " 2   body          80371 non-null  object \n",
      " 3   upvotes       98826 non-null  float64\n",
      " 4   created_utc   98826 non-null  float64\n",
      " 5   num_comments  98826 non-null  float64\n",
      " 6   label         98826 non-null  float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Now you can fill missing values in 'body' column \n",
    "data['num_comments'] = data['num_comments'].fillna(0)\n",
    "\n",
    "# Check the DataFrame info again to confirm the change\n",
    "data.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have filled the missing values in the 'num_comments' column with 0, but the 'body' column still has missing values. \n",
    "\n",
    "After filling 'num_comments', the DataFrame now shows that 'num_comments' has no missing values, but the 'body' column still has 18,455 missing entries (i.e., approximately 18.67% of the rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit           0\n",
       "title               0\n",
       "body            18455\n",
       "upvotes             0\n",
       "created_utc         0\n",
       "num_comments        0\n",
       "label               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The'body' column has 18,455 missing values, while other columns are free of missing data.\n",
    "\n",
    "This indicates that a significant portion of text data in the 'body' column is absent, which could affect downstream analysis.\n",
    " We may need to fill or handle these missing values appropriately before proceeding with further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98826 entries, 0 to 98825\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   subreddit     98826 non-null  object \n",
      " 1   title         98826 non-null  object \n",
      " 2   body          98826 non-null  object \n",
      " 3   upvotes       98826 non-null  float64\n",
      " 4   created_utc   98826 non-null  float64\n",
      " 5   num_comments  98826 non-null  float64\n",
      " 6   label         98826 non-null  float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Now you can fill missing values in 'body' column \n",
    "data['body'] = data['body'].fillna(\" \")\n",
    "\n",
    "# Check the DataFrame info again to confirm the change\n",
    "data.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have filled the missing values in the 'body' column with empty strings \"\". The dataset now has 98826 rows with no missing values in any column now. The dataset is ready for analysis, with all columns appropriately formatted for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit       0\n",
       "title           0\n",
       "body            0\n",
       "upvotes         0\n",
       "created_utc     0\n",
       "num_comments    0\n",
       "label           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for missing values \n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirmed all the columns have no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>date</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Day 18 of doing 50 push-ups</td>\n",
       "      <td></td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-04-25 13:25:39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>There isnâ€™t a better feeling than finishing yo...</td>\n",
       "      <td>Then your teacher hits you with that â€œ Good jo...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2019-04-01 07:19:57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>LMAOOO I can only get this guy to talk to me i...</td>\n",
       "      <td>Yeahhh maybe not babe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-04-07 14:35:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>This isn't going to work out</td>\n",
       "      <td></td>\n",
       "      <td>236.0</td>\n",
       "      <td>2014-12-03 18:12:52</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Am I the only guy who found it hurtful as a ch...</td>\n",
       "      <td>\\n\\nLike... why? How is that funny? How does ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-09-23 23:07:59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98821</th>\n",
       "      <td>depression</td>\n",
       "      <td>Has anyone ever had any problems with gangs?</td>\n",
       "      <td>I don't care if my grammer isn't good so dont ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2019-05-17 08:30:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98822</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>Moments away from killing myself, not even bot...</td>\n",
       "      <td>I don't want to talk about my problems, I don'...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2012-10-20 17:24:21</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98823</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I've grown used to being like this for years.</td>\n",
       "      <td>I'm not suicidal at the moment, in fact I feel...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2015-05-19 22:19:26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98824</th>\n",
       "      <td>depression</td>\n",
       "      <td>This speech from The Lord of the Rings really ...</td>\n",
       "      <td>\"It's like in the great stories, Mr. Frodo. Th...</td>\n",
       "      <td>323.0</td>\n",
       "      <td>2012-12-16 15:44:18</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98825</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I just don't know....</td>\n",
       "      <td>I have never thought that I will end up in thi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2012-02-04 11:36:07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98826 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit                                              title  \\\n",
       "0         teenagers                        Day 18 of doing 50 push-ups   \n",
       "1         teenagers  There isnâ€™t a better feeling than finishing yo...   \n",
       "2         teenagers  LMAOOO I can only get this guy to talk to me i...   \n",
       "3         teenagers                       This isn't going to work out   \n",
       "4         teenagers  Am I the only guy who found it hurtful as a ch...   \n",
       "...             ...                                                ...   \n",
       "98821    depression       Has anyone ever had any problems with gangs?   \n",
       "98822  SuicideWatch  Moments away from killing myself, not even bot...   \n",
       "98823  SuicideWatch      I've grown used to being like this for years.   \n",
       "98824    depression  This speech from The Lord of the Rings really ...   \n",
       "98825  SuicideWatch                              I just don't know....   \n",
       "\n",
       "                                                    body  upvotes  \\\n",
       "0                                                             4.0   \n",
       "1      Then your teacher hits you with that â€œ Good jo...      7.0   \n",
       "2                                  Yeahhh maybe not babe      4.0   \n",
       "3                                                           236.0   \n",
       "4       \\n\\nLike... why? How is that funny? How does ...      6.0   \n",
       "...                                                  ...      ...   \n",
       "98821  I don't care if my grammer isn't good so dont ...      8.0   \n",
       "98822  I don't want to talk about my problems, I don'...     12.0   \n",
       "98823  I'm not suicidal at the moment, in fact I feel...     14.0   \n",
       "98824  \"It's like in the great stories, Mr. Frodo. Th...    323.0   \n",
       "98825  I have never thought that I will end up in thi...      5.0   \n",
       "\n",
       "                     date  num_comments  label  \n",
       "0     2021-04-25 13:25:39           4.0    0.0  \n",
       "1     2019-04-01 07:19:57           0.0    0.0  \n",
       "2     2022-04-07 14:35:00          12.0    0.0  \n",
       "3     2014-12-03 18:12:52          33.0    0.0  \n",
       "4     2019-09-23 23:07:59           4.0    0.0  \n",
       "...                   ...           ...    ...  \n",
       "98821 2019-05-17 08:30:19           1.0    1.0  \n",
       "98822 2012-10-20 17:24:21           8.0    1.0  \n",
       "98823 2015-05-19 22:19:26           4.0    1.0  \n",
       "98824 2012-12-16 15:44:18          28.0    1.0  \n",
       "98825 2012-02-04 11:36:07           3.0    1.0  \n",
       "\n",
       "[98826 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'created_utc' column from UTC epoch time to datetime\n",
    "data['created_utc'] = pd.to_datetime(data['created_utc'], unit='s')\n",
    "\n",
    "\n",
    "# Rename the column 'created_utc' to 'date'\n",
    "data = data.rename(columns={'created_utc': 'date'})\n",
    "\n",
    "# To see the updated DataFrame\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'created_utc' column has been successfully converted from UTC epoch time to a readable datetime format, and it has been renamed to 'date'. Now, the DataFrame includes a 'date' column with the datetime representation of when each post was created, which makes the dataset more user-friendly for time-based analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit       0\n",
       "title           0\n",
       "body            0\n",
       "upvotes         0\n",
       "date            0\n",
       "num_comments    0\n",
       "label           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for missing values \n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>date</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>label</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>upvotes_per_comment</th>\n",
       "      <th>has_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Day 18 of doing 50 push-ups</td>\n",
       "      <td></td>\n",
       "      <td>-0.062214</td>\n",
       "      <td>2021-04-25 13:25:39</td>\n",
       "      <td>-0.150176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>-0.239286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>There isnâ€™t a better feeling than finishing yo...</td>\n",
       "      <td>Then your teacher hits you with that â€œ Good jo...</td>\n",
       "      <td>-0.059013</td>\n",
       "      <td>2019-04-01 07:19:57</td>\n",
       "      <td>-0.207278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.775017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>LMAOOO I can only get this guy to talk to me i...</td>\n",
       "      <td>Yeahhh maybe not babe</td>\n",
       "      <td>-0.062214</td>\n",
       "      <td>2022-04-07 14:35:00</td>\n",
       "      <td>-0.035970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>-0.319826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>This isn't going to work out</td>\n",
       "      <td></td>\n",
       "      <td>0.185294</td>\n",
       "      <td>2014-12-03 18:12:52</td>\n",
       "      <td>0.263818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.765393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Am I the only guy who found it hurtful as a ch...</td>\n",
       "      <td>\\n\\nLike... why? How is that funny? How does ...</td>\n",
       "      <td>-0.060080</td>\n",
       "      <td>2019-09-23 23:07:59</td>\n",
       "      <td>-0.150176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.173847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98821</th>\n",
       "      <td>depression</td>\n",
       "      <td>Has anyone ever had any problems with gangs?</td>\n",
       "      <td>I don't care if my grammer isn't good so dont ...</td>\n",
       "      <td>-0.057946</td>\n",
       "      <td>2019-05-17 08:30:19</td>\n",
       "      <td>-0.193002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.284225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98822</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>Moments away from killing myself, not even bot...</td>\n",
       "      <td>I don't want to talk about my problems, I don'...</td>\n",
       "      <td>-0.053679</td>\n",
       "      <td>2012-10-20 17:24:21</td>\n",
       "      <td>-0.093073</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>-0.152034</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98823</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I've grown used to being like this for years.</td>\n",
       "      <td>I'm not suicidal at the moment, in fact I feel...</td>\n",
       "      <td>-0.051545</td>\n",
       "      <td>2015-05-19 22:19:26</td>\n",
       "      <td>-0.150176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.087908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98824</th>\n",
       "      <td>depression</td>\n",
       "      <td>This speech from The Lord of the Rings really ...</td>\n",
       "      <td>\"It's like in the great stories, Mr. Frodo. Th...</td>\n",
       "      <td>0.278110</td>\n",
       "      <td>2012-12-16 15:44:18</td>\n",
       "      <td>0.192440</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>1.451971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98825</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I just don't know....</td>\n",
       "      <td>I have never thought that I will end up in thi...</td>\n",
       "      <td>-0.061147</td>\n",
       "      <td>2012-02-04 11:36:07</td>\n",
       "      <td>-0.164451</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>-0.165667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98826 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit                                              title  \\\n",
       "0         teenagers                        Day 18 of doing 50 push-ups   \n",
       "1         teenagers  There isnâ€™t a better feeling than finishing yo...   \n",
       "2         teenagers  LMAOOO I can only get this guy to talk to me i...   \n",
       "3         teenagers                       This isn't going to work out   \n",
       "4         teenagers  Am I the only guy who found it hurtful as a ch...   \n",
       "...             ...                                                ...   \n",
       "98821    depression       Has anyone ever had any problems with gangs?   \n",
       "98822  SuicideWatch  Moments away from killing myself, not even bot...   \n",
       "98823  SuicideWatch      I've grown used to being like this for years.   \n",
       "98824    depression  This speech from The Lord of the Rings really ...   \n",
       "98825  SuicideWatch                              I just don't know....   \n",
       "\n",
       "                                                    body   upvotes  \\\n",
       "0                                                        -0.062214   \n",
       "1      Then your teacher hits you with that â€œ Good jo... -0.059013   \n",
       "2                                  Yeahhh maybe not babe -0.062214   \n",
       "3                                                         0.185294   \n",
       "4       \\n\\nLike... why? How is that funny? How does ... -0.060080   \n",
       "...                                                  ...       ...   \n",
       "98821  I don't care if my grammer isn't good so dont ... -0.057946   \n",
       "98822  I don't want to talk about my problems, I don'... -0.053679   \n",
       "98823  I'm not suicidal at the moment, in fact I feel... -0.051545   \n",
       "98824  \"It's like in the great stories, Mr. Frodo. Th...  0.278110   \n",
       "98825  I have never thought that I will end up in thi... -0.061147   \n",
       "\n",
       "                     date  num_comments  label      year     month       day  \\\n",
       "0     2021-04-25 13:25:39     -0.150176    0.0  0.928571  0.272727  0.800000   \n",
       "1     2019-04-01 07:19:57     -0.207278    0.0  0.785714  0.272727  0.000000   \n",
       "2     2022-04-07 14:35:00     -0.035970    0.0  1.000000  0.272727  0.200000   \n",
       "3     2014-12-03 18:12:52      0.263818    0.0  0.428571  1.000000  0.066667   \n",
       "4     2019-09-23 23:07:59     -0.150176    0.0  0.785714  0.727273  0.733333   \n",
       "...                   ...           ...    ...       ...       ...       ...   \n",
       "98821 2019-05-17 08:30:19     -0.193002    1.0  0.785714  0.363636  0.533333   \n",
       "98822 2012-10-20 17:24:21     -0.093073    1.0  0.285714  0.818182  0.633333   \n",
       "98823 2015-05-19 22:19:26     -0.150176    1.0  0.500000  0.363636  0.600000   \n",
       "98824 2012-12-16 15:44:18      0.192440    1.0  0.285714  1.000000  0.500000   \n",
       "98825 2012-02-04 11:36:07     -0.164451    1.0  0.285714  0.090909  0.100000   \n",
       "\n",
       "           hour  upvotes_per_comment  has_body  \n",
       "0      0.565217            -0.239286         0  \n",
       "1      0.304348             0.775017         1  \n",
       "2      0.608696            -0.319826         1  \n",
       "3      0.782609             0.765393         0  \n",
       "4      1.000000            -0.173847         1  \n",
       "...         ...                  ...       ...  \n",
       "98821  0.347826             0.284225         1  \n",
       "98822  0.739130            -0.152034         1  \n",
       "98823  0.956522             0.087908         1  \n",
       "98824  0.652174             1.451971         1  \n",
       "98825  0.478261            -0.165667         1  \n",
       "\n",
       "[98826 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- 1. Feature Creation ---\n",
    "\n",
    "# Extract year, month, day, hour from 'date'\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['hour'] = data['date'].dt.hour\n",
    "\n",
    "# Create additional features like ratios or flags\n",
    "data['upvotes_per_comment'] = data['upvotes'] / (data['num_comments'] + 1)  # Adding 1 to avoid division by zero\n",
    "data['has_body'] = data['body'].apply(lambda x: 0 if x == ' ' else 1)  # Flag indicating if there's content in 'body'\n",
    "\n",
    "\n",
    "# --- 2. Scaling and Normalization ---\n",
    "# Initialize scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Standard Scaling on numerical features\n",
    "data[['upvotes', 'num_comments', 'upvotes_per_comment']] = standard_scaler.fit_transform(\n",
    "    data[['upvotes', 'num_comments', 'upvotes_per_comment']]\n",
    ")\n",
    "\n",
    "# Apply Min-Max Scaling on year, month, day, hour (optional)\n",
    "data[['year', 'month', 'day', 'hour']] = minmax_scaler.fit_transform(\n",
    "    data[['year', 'month', 'day', 'hour']]\n",
    ")\n",
    "\n",
    "# Display the final DataFrame\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame now includes 12 columns with additional features like the extracted year, month, day, and hour from the date column, along with calculated features such as upvotes_per_comment and has_body. \n",
    "\n",
    "These transformations include scaling numerical variables and handling missing values, providing a well-prepared dataset for further analysis or machine learning tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98826 entries, 0 to 98825\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   subreddit            98826 non-null  object        \n",
      " 1   title                98826 non-null  object        \n",
      " 2   body                 98826 non-null  object        \n",
      " 3   upvotes              98826 non-null  float64       \n",
      " 4   date                 98826 non-null  datetime64[ns]\n",
      " 5   num_comments         98826 non-null  float64       \n",
      " 6   label                98826 non-null  float64       \n",
      " 7   year                 98826 non-null  float64       \n",
      " 8   month                98826 non-null  float64       \n",
      " 9   day                  98826 non-null  float64       \n",
      " 10  hour                 98826 non-null  float64       \n",
      " 11  upvotes_per_comment  98826 non-null  float64       \n",
      " 12  has_body             98826 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(8), int64(1), object(3)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirmed the dataframe has 12 columns as listed above, including both original and newly created features. All columns have non-null values, and the data types are appropriately set for each feature, with numeric and categorical variables ready for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GICHEHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GICHEHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GICHEHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subreddit                                    processed_title  \\\n",
      "0  teenagers                                   day 18 50 pushup   \n",
      "1  teenagers  â€™ better feeling finishing science paper 1am d...   \n",
      "2  teenagers                     lmaooo get guy talk talk godðŸ’–ðŸ’–   \n",
      "3  teenagers                                    isnt going work   \n",
      "4  teenagers  guy found hurtful child movie kid female chara...   \n",
      "\n",
      "                                      processed_body  \n",
      "0                                                     \n",
      "1  teacher hit â€œ good job everyone draft final co...  \n",
      "2                                  yeahhh maybe babe  \n",
      "3                                                     \n",
      "4  like funny deserve better male character kick ...  \n"
     ]
    }
   ],
   "source": [
    "#Text Preprocessing \n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import string\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stopwords, punctuation, and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a class TextPreprocessor that conforms to scikit-learnâ€™s transformer API\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting required for this transformer\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Ensure X is a pandas Series and apply preprocessing\n",
    "        if isinstance(X, pd.Series):\n",
    "            return X.apply(self._preprocess_text)\n",
    "        elif isinstance(X, (list, np.ndarray)):\n",
    "            return [self._preprocess_text(text) for text in X]\n",
    "        else:\n",
    "            raise ValueError(\"Input should be a pandas Series, list, or numpy array\")\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        if pd.isnull(text):\n",
    "            return ''  # Return empty string for missing values\n",
    "        text = text.lower()  # Lowercase\n",
    "        text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "        tokens = word_tokenize(text)  # Tokenize\n",
    "        tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize\n",
    "        return ' '.join(tokens)  # Join tokens back into string\n",
    "\n",
    "# Instantiate TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor()\n",
    "\n",
    "# Assuming 'data' is your pandas DataFrame and it contains 'title' and 'body' columns\n",
    "data['processed_title'] = text_preprocessor.transform(data['title'])\n",
    "data['processed_body'] = text_preprocessor.transform(data['body'])\n",
    "\n",
    "# Display the processed data\n",
    "print(data[['subreddit', 'processed_title', 'processed_body']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data in the processed_title and processed_body columns has been cleaned by removing stopwords, punctuation, and irrelevant characters, while lemmatizing words. This has resulted to a simplified and standardized text, making it more suitable for further analysis like classification or sentiment analysis. For example, titles like \"Day 18 of doing 50 push-ups\" are reduced to \"day 18 50 pushup,\" improving consistency across the dataset.\n",
    "\n",
    "We will produce  numerical representation of the text data, enabling us to use it as input for machine learning models.\n",
    "\n",
    "We will use TF-IDF as it weighs words by their frequency in a document relative to their frequency across all documents.\n",
    "\n",
    "It allows to leverage the importance of unique words within each post and balances the contribution of frequent words, making it well-suited for distinguishing between subreddits or post types.\n",
    "\n",
    "starting with TF-IDF and evaluate its performance. If the results arenâ€™t satisfactory, we will consider experimenting with word embeddings for potential improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Representation into numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF -give higher weights to words that are less common across documents.\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Apply TF-IDF to the title and body columns separately\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # limit to top 10,000 features_to manage vocabulary size\n",
    "\n",
    "\n",
    "# Use the processed title and body columns \n",
    "tfidf_title = tfidf_vectorizer.fit_transform(data['processed_title'])\n",
    "tfidf_body = tfidf_vectorizer.fit_transform(data['processed_body'])\n",
    "\n",
    "# You can keep these as sparse matrices or concatenate directly if needed for model input\n",
    "combined_features = csr_matrix(tfidf_title) + csr_matrix(tfidf_body) # in sparse format, to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have feature reduction where we will be working with the top 10,000 - retaining the most relevant words, reducing noise and memory load.\n",
    "\n",
    "If we use all features, We get a memory error indicating that the conversion of the TF-IDF sparse matrix to a dense format (.toarray()) is exceeding the available memory, likely due to the large number of documents and vocabulary size.\n",
    "\n",
    "Hence the Sparse Matrix Size: The resulting TF-IDF matrix will have a maximum of 10,000 columns (features), significantly reducing memory requirements when compared to the full vocabulary size.\n",
    "\n",
    "This compromise allows us to handle large datasets while retaining a high level of detail in the features for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>im</td>\n",
       "      <td>2951.154806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9640</th>\n",
       "      <td>want</td>\n",
       "      <td>2920.344788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5264</th>\n",
       "      <td>like</td>\n",
       "      <td>2674.925609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4540</th>\n",
       "      <td>ie</td>\n",
       "      <td>1887.889586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>dont</td>\n",
       "      <td>1867.329365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082</th>\n",
       "      <td>know</td>\n",
       "      <td>1829.199974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>feel</td>\n",
       "      <td>1785.575874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3924</th>\n",
       "      <td>get</td>\n",
       "      <td>1679.820679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165</th>\n",
       "      <td>one</td>\n",
       "      <td>1572.184302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>life</td>\n",
       "      <td>1482.131980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5150</th>\n",
       "      <td>launch</td>\n",
       "      <td>1461.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>friend</td>\n",
       "      <td>1426.191804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6446</th>\n",
       "      <td>people</td>\n",
       "      <td>1407.786398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7139</th>\n",
       "      <td>really</td>\n",
       "      <td>1405.373673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>time</td>\n",
       "      <td>1372.134384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5135</th>\n",
       "      <td>last</td>\n",
       "      <td>1369.205161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>grain</td>\n",
       "      <td>1316.106927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4227</th>\n",
       "      <td>he</td>\n",
       "      <td>1290.570193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>day</td>\n",
       "      <td>1269.633881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>go</td>\n",
       "      <td>1232.312702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term  tfidf_score\n",
       "4558      im  2951.154806\n",
       "9640    want  2920.344788\n",
       "5264    like  2674.925609\n",
       "4540      ie  1887.889586\n",
       "2835    dont  1867.329365\n",
       "5082    know  1829.199974\n",
       "3500    feel  1785.575874\n",
       "3924     get  1679.820679\n",
       "6165     one  1572.184302\n",
       "5247    life  1482.131980\n",
       "5150  launch  1461.349998\n",
       "3774  friend  1426.191804\n",
       "6446  people  1407.786398\n",
       "7139  really  1405.373673\n",
       "9002    time  1372.134384\n",
       "5135    last  1369.205161\n",
       "4028   grain  1316.106927\n",
       "4227      he  1290.570193\n",
       "2426     day  1269.633881\n",
       "3973      go  1232.312702"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examine the top words by TF-IDF Scores\n",
    "\n",
    "# Display the top terms with the highest TF-IDF scores\n",
    "tfidf_sum = combined_features.sum(axis=0)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame({'term': feature_names, 'tfidf_score': tfidf_sum.A1})\n",
    "tfidf_df = tfidf_df.sort_values(by='tfidf_score', ascending=False)\n",
    "\n",
    "# Display the top 20 terms with the highest TF-IDF scores\n",
    "tfidf_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top terms with the highest TF-IDF scores from the dataset reveal a few key observations. Terms such as \"im,\" \"want,\" \"like,\" \"dont,\" and \"know\" dominate the list, with scores ranging from approximately 2900 to 1200. These words are common in casual conversations or personal reflections, which aligns with the informal nature of the data (from forums like Reddit). Other notable terms include \"friend,\" \"feel,\" \"life,\" \"time,\" and \"people,\" which suggest emotional and social themes are prominent in the posts.\n",
    "\n",
    "By looking at the TF-IDF scores, we can infer that the dataset likely contains a mix of self-reflective or emotional content (e.g., terms like \"feel,\" \"know,\" \"life\") and expressions commonly used in digital communication (e.g., \"dont,\" \"like,\" \"want\"). This information can guide further text analysis or classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top terms with the highest TF-IDF scores from the dataset reveal a few key observations. Terms such as \"im,\" \"want,\" \"like,\" \"dont,\" and \"know\" dominate the list, with scores ranging from approximately 2900 to 1200. These words are common in casual conversations or personal reflections, which aligns with the informal nature of the data (from forums like Reddit). Other notable terms include \"friend,\" \"feel,\" \"life,\" \"time,\" and \"people,\" which suggest emotional and social themes are prominent in the posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Vader cannot handle well domain specific context(mental health content)  and cannot learn patterns beyond its lexicon or adapt to the datasets nuances, we will train a machine learning model to classify the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8982544902605616\n",
      "Testing Accuracy: 0.8835373874329657\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.88      0.92     15998\n",
      "         1.0       0.64      0.90      0.75      3768\n",
      "\n",
      "    accuracy                           0.88     19766\n",
      "   macro avg       0.81      0.89      0.84     19766\n",
      "weighted avg       0.91      0.88      0.89     19766\n",
      "\n",
      "Pipeline saved successfully!\n"
     ]
    }
   ],
   "source": [
    "#Select and train a model using features extracted in the previous step.\n",
    "# Model 1 - Baseline Model \n",
    "#Logistic Regression from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.pipeline import Pipeline  # Use imbalanced-learn's Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Combine the title and body text into one column\n",
    "data['combined_text'] = data['title'].fillna('') + ' ' + data['body'].fillna('')\n",
    "\n",
    "# Define the target and features\n",
    "X = data['combined_text']\n",
    "y = data['label']\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a pipeline with text preprocessing, TF-IDF vectorization, SMOTE, and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000)),  # Vectorization\n",
    "    ('smote', SMOTE(random_state=42)),  # Handle class imbalance\n",
    "    ('classifier', LogisticRegression(solver='saga', max_iter=200))  # Classifier\n",
    "])\n",
    "\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on training data\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Evaluate on testing data\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# Print classification report for the test set\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, test_predictions))\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(pipeline, 'models/sentiments_pipeline.pkl')\n",
    "print(\"Pipeline saved successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GICHEHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\GICHEHA\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Example for punkt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"This is a test sentence.\"\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: sample sentence need cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Test TextPreprocessor class independently\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample sentence that needs to be cleaned.\"\n",
    "\n",
    "# Tokenize and process text\n",
    "tokens = text.lower().split()\n",
    "cleaned_text = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "print(\"Cleaned text:\", ' '.join(cleaned_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first model, we used logistic regression to model our data. We addressed the identified class imbalance using SMOTE and used TF-IDF to extract features from the text.\n",
    "\n",
    "The model performed well, achieving a training accuracy of 92.8% and a testing accuracy of 91.8%.\n",
    "\n",
    " For non-depressed posts (label 0), the precision was 0.98, meaning the model was very accurate at predicting these posts, and the recall was 0.92, indicating it identified most of them correctly.\n",
    "\n",
    " For depressed posts (label 1), the precision was 0.73, showing moderate accuracy in identifying depressed posts, and the recall was 0.91, meaning the model was good at catching most of these posts, though with slightly lower precision.\n",
    "\n",
    "Overall, while the model achieved high accuracy, thereâ€™s a slight compromise between precision and recall for the depressed posts. The f1-score for depressed posts is 0.81, reflecting a good balance between precision and recall. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Predictions on the test set\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "# Visualize the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-depressed (0)', 'Depressed (1)'], \n",
    "            yticklabels=['Non-depressed (0)', 'Depressed (1)'])\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the confusion Matrix\n",
    "\n",
    "When we trained the model, we got the following results\n",
    "\n",
    "1. (True Negatives )14,733 labels were correctly classified as 0 (non depressed)\n",
    "\n",
    "2. (True Positives) 3417 labels were correctly classified as 1 (depressed)\n",
    "\n",
    "3. (False Negatives) 351 labels were incorrectly classified as 0 (non depressed) but were actually depressed 1 - Missed cases\n",
    "\n",
    "4. (False Positives) 1265 labels were incorrectly classified as depressed but were actually non depressed - False Alarms\n",
    "\n",
    "The confusion matrix shows the model is highly accurate overall but may have room to improve in correctly identifying depressive posts,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next steps, we will attempt to further improve the model through hyperparameter tuning, additional feature engineering, and by exploring more advanced models like BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning and Optimization Hyperparameter Tuning - Randomized search CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=50. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "80 fits failed out of a total of 80.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\utils\\fixes.py\", line 85, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\", line 255, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\", line 1104, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\", line 855, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 806, in fit\n",
      "    return self.partial_fit(X, y, sample_weight)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 841, in partial_fit\n",
      "    X = self._validate_data(\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\", line 566, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 746, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\series.py\", line 917, in __array__\n",
      "    arr = np.asarray(values, dtype=dtype)\n",
      "ValueError: could not convert string to float: \"How did y'all come up with your usernames? Mine is from Simon and Garfunkle.\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "64 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\utils\\fixes.py\", line 85, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\", line 329, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\", line 255, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\", line 1104, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\", line 855, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 806, in fit\n",
      "    return self.partial_fit(X, y, sample_weight)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 841, in partial_fit\n",
      "    X = self._validate_data(\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\", line 566, in _validate_data\n",
      "    X = check_array(X, **check_params)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 746, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\series.py\", line 917, in __array__\n",
      "    arr = np.asarray(values, dtype=dtype)\n",
      "ValueError: could not convert string to float: 'Am I the stupid? I have an extremely similar name to another person at my school. We\\'re at an awards assembly and I hear \"Insert name\" and am confused if I should go up or not. So, I go up, turns out it wasn\\'t me who was supposed to go. I leave the assembly and am immediately being put on blast by literally anyone present. I feel like I\\'m not in the wrong here.'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:984: RuntimeWarning: invalid value encountered in cast\n",
      "  results[\"rank_%s\" % key_name] = np.asarray(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Am I the stupid? I have an extremely similar name to another person at my school. We\\'re at an awards assembly and I hear \"Insert name\" and am confused if I should go up or not. So, I go up, turns out it wasn\\'t me who was supposed to go. I leave the assembly and am immediately being put on blast by literally anyone present. I feel like I\\'m not in the wrong here.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-aa9fcf1d302d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Fit the RandomizedSearchCV object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Get the best parameters and score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\utils\\fixes.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     )\n\u001b[0;32m     84\u001b[0m                 ):\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \"\"\"\n\u001b[0;32m    328\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fit\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             ):\n\u001b[1;32m--> 255\u001b[1;33m                 X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    256\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    853\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m    840\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_samples_seen_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m    842\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32mc:\\Users\\GICHEHA\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \"\"\"\n\u001b[0;32m    916\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Am I the stupid? I have an extremely similar name to another person at my school. We\\'re at an awards assembly and I hear \"Insert name\" and am confused if I should go up or not. So, I go up, turns out it wasn\\'t me who was supposed to go. I leave the assembly and am immediately being put on blast by literally anyone present. I feel like I\\'m not in the wrong here.'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'model__C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'model__penalty': ['l1', 'l2'],       # Regularization types\n",
    "    'model__solver': ['liblinear', 'saga']  # Compatible solvers\n",
    "}\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_dist,\n",
    "    n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV object\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = random_search.best_estimator_.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Testing Accuracy with Best Parameters:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the model performs with an accuracy of about 54% across the cross-validation folds. While this is an improvement over random guessing, it suggests that there might be room for improvement in terms of feature engineering, model selection, or hyperparameter tuning.\n",
    "\n",
    "The accuracy on the test set is 51%, which is slightly above random guessing (50%) in a binary classification task. This indicates that while the model is doing somewhat better than chance, there is still potential for improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use other models such as a Random Forest to see whether they will perform better for the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Random Forest\n",
      "Training Accuracy (Random Forest): 0.9999747027573994\n",
      "Testing Accuracy (Random Forest): 0.9055448750379439\n",
      "Classification Report (Random Forest):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.95      0.94     15998\n",
      "         1.0       0.76      0.73      0.75      3768\n",
      "\n",
      "    accuracy                           0.91     19766\n",
      "   macro avg       0.85      0.84      0.84     19766\n",
      "weighted avg       0.90      0.91      0.90     19766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Combine the title and body text into one column\n",
    "data['combined_text'] = data['title'].fillna('') + ' ' + data['body'].fillna('')\n",
    "\n",
    "# Define the target and features\n",
    "X = data[['combined_text']]\n",
    "y = data['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing: text vectorization only\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', TfidfVectorizer(max_features=10000), 'combined_text')  # Text processing\n",
    "])\n",
    "\n",
    "# Define the models to use\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    #'SVM': SVC(kernel='linear', probability=True, random_state=42)\n",
    "    \n",
    "}\n",
    "\n",
    "# Loop through each model and evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    train_predictions = pipeline.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    print(f\"Training Accuracy ({model_name}):\", train_accuracy)\n",
    "\n",
    "    # Evaluate on testing data\n",
    "    test_predictions = pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    print(f\"Testing Accuracy ({model_name}):\", test_accuracy)\n",
    "\n",
    "    # Print classification report for test set\n",
    "    print(f\"Classification Report ({model_name}):\\n\", classification_report(y_test, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
